{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4cdcc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rosa/.local/lib/python3.10/site-packages/anaconda3/envs/cgt-project-example/lib/python3.8/site-packages/pycolab/ascii_art.py:318: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  art = np.vstack(np.fromstring(line, dtype=np.uint8) for line in art)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import environment\n",
    "import model\n",
    "\n",
    "n_agents = 2\n",
    "env = environment.CommonPoolEnv(model.DEFAULT_MAP, n_agents)\n",
    "observations, *_ = env.reset()\n",
    "observation = observations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf41894d-3da2-409b-a5e1-321161f05ede",
   "metadata": {},
   "source": [
    "1) EXPERIENCE REPLAY:\n",
    "   randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution\n",
    "2) Iterative update:\n",
    "   adjust tge action-values (Q) towards target values that are only periodically updated, thereby reducing the correlations with the target\n",
    "   Dit betekend dat de target DQN pas om de zoveel iteraties wordt geupdate. Het training DQN elke keer (zo neem ik het aan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6279b06a-ddf2-4e30-be2b-07fe4f5cef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# if GPU is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77237217-e3ea-4e11-92b3-31a42bb4ba77",
   "metadata": {},
   "source": [
    "Ben aan de hand hiervan beginnen proberen\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1d8315-e54d-49c3-9de1-13413253cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity, minibatch_size=32):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "        self._index = (self._index + 1)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "    def current_index(self):\n",
    "            return self._index\n",
    "\n",
    "    def get_minibatch_indices(self):\n",
    "            indices = []\n",
    "            while len(indices) < minibatch_size:\n",
    "                index = np.random.randint(low=0, high = self.current_index, dtype=np.int32)\n",
    "                indices.append(index)\n",
    "\n",
    "    def generate_minibatch_samples(self, indices):\n",
    "        state_batch, action_batch, next_state_batch, reward_batch = [], [], [], []\n",
    "    \n",
    "        for index in indices:\n",
    "            selected_mem = self._memory[index]\n",
    "            state_batch.append(torch.tensor(selected_mem.state, dtype=torch.float32))\n",
    "            action_batch.append(torch.tensor(selected_mem.action, dtype=torch.int32))\n",
    "            next_state_batch.append(torch.tensor(selected_mem.next_state, dtype=torch.float32))\n",
    "            reward_batch.append(torch.tensor(selected_mem.reward, dtype=torch.float32))\n",
    "            \n",
    "        return torch.stack(state_batch, dim=0), torch.stack(action_batch, dim=0), torch.stack(next_state_batch, dim=0), torch.stack(reward_batch, dim=0)\n",
    "\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02113e83-3d75-423a-bbc2-87b11cc6d946",
   "metadata": {},
   "source": [
    "Hier beetje aan het prutsen geweest met CNN, maar wou nu eerst DQN te laten werken door gewoon te flatten en zo te trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab7afbe9-4607-44c6-aca3-5b0df1d117b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 3 * environment.OBSERVE_WIDTH_VIEW * environment.OBSERVE_FRONT_VIEW\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "padding = 0\n",
    "output_size = ((input_size - kernel_size + 2 * padding) / stride) + 1\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48591f1e-125e-4479-8eba-00a286074969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1252.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = output_size\n",
    "kernel_size = 5\n",
    "stride = 1\n",
    "padding = 0\n",
    "output_size = ((input_size - kernel_size + 2 * padding) / stride) + 1\n",
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a95fe6-351c-45e1-8464-2d453cdcd650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnDQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=model.N_ACTIONS):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1)\n",
    "        \n",
    "        self.layer1 = nn.Linear(412, hidden_size)    # Input Layer\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)   # Hidden layer 1\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)   # Hidden Layer 2\n",
    "        self.layer4 = nn.Linear(hidden_size, output_size)   # Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bce7247-143b-4047-a8b3-b980a53dc944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN zoals in onze paper staat\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=model.N_ACTIONS):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)    # Input Layer\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)   # Hidden layer 1\n",
    "        self.layer3 = nn.Linear(hidden_size, hidden_size)   # Hidden Layer 2\n",
    "        self.layer4 = nn.Linear(hidden_size, output_size)   # Output Layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer3(x))\n",
    "        return self.layer4(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e19fe354-d48f-490f-82b9-a523752b56af",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95d42b69-0f59-4554-85b1-5a02df67bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "BATCH_SIZE = 32    # Minibatch size\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1\n",
    "EPS_END = 0.1\n",
    "EPS_DECAY = 10000  #LINEARLY OVER TIME?\n",
    "LR = 0.00025 # from paper human action\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "observation = env.reset()\n",
    "\n",
    "n_observations = len(observation[0])\n",
    "\n",
    "policy_net = DQN(363).to(device)\n",
    "target_net = DQN(363).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer1 = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "# Policy van paper is RMSPROP\n",
    "#optimizer = optim.RMSProp(policy_net.parameters(),lr=LR, momentum=0.95,...)\n",
    "\n",
    "\n",
    "memory = ReplayMemory(10000) # Paper Human... has it on 1 000 000\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state, epsilon):\n",
    "    global steps_done\n",
    "    if random.random() > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state) # tensor([-0.0620,  0.0564,  0.0095, -0.1008, -0.2029,  0.1698,  0.1158, -0.1945]) argmax ofzo hier nog\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "66c5fb04-d426-4d25-828d-f881d57d3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = torch.tensor([[env.action_space.sample()]])\n",
    "nObservations, nRewards, nDone, nInfo = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a626794e-44bc-4d31-86cb-9946c06df84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action tensor([[7]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.74901961, 0.74901961, 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.74901961, 0.74901961, 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.74901961, 0.74901961, 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.74901961, 0.74901961, 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.74901961, 0.74901961, 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"action {action}\")\n",
    "nObservations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e63f9-b0b3-4f08-9b22-7c66be5a7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43d53f0-d49d-48be-9e72-fd086de44921",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mReplayMemory.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/final/lib/python3.8/random.py:363\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k)\u001b[0m\n\u001b[1;32m    361\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(population)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m k \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n:\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample larger than population or is negative\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m result \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m k\n\u001b[1;32m    365\u001b[0m setsize \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "total_steps = 0\n",
    "episode = 0\n",
    "\n",
    "nStates = env.reset()\n",
    "while total_step < training_steps\n",
    "    \n",
    "    state_agent0 = torch.tensor(nStates[0], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    flatten_state = torch.flatten(state_agent0)\n",
    "\n",
    "    epsilon = 1\n",
    "\n",
    "    action = select_action(flatten_state, epsilon)\n",
    "    next_state, reward, done, info = env.step(action)\n",
    "    episode_score += reward[0]\n",
    "    # memory = ('state', 'action', 'next_state', 'reward'))\n",
    "    memory.push(nStates[0], action, next_state[0], reward[0])\n",
    "    nStates = next_state\n",
    "\n",
    "    \n",
    "    if (total_step % update_frequency == 0):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1d636cb-66ac-4492-be0b-c522b62bf95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee612c92-2ad5-495a-ae66-5d204c756c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "state = torch.tensor(obs[0], dtype=torch.float32, device=device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56841ab8-dd49-446e-bf8f-2477095397d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_state = torch.flatten(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5369198f-6c75-49b5-8fb1-8ad90c7790ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([363])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d19e811-613f-49dc-acae-92ee349939a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "act = policy_net(flatten_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad476344-c0ad-4438-a494-97fd55c93c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0620,  0.0564,  0.0095, -0.1008, -0.2029,  0.1698,  0.1158, -0.1945],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66c71d29-8662-403f-86ba-7d92ceb5b12d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'tonumpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtonumpy\u001b[49m())\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'tonumpy'"
     ]
    }
   ],
   "source": [
    "np.argmax(policy_net(flatten_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82b923ca-e458-4b9c-b0a1-b4a495825b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0620,  0.0564,  0.0095, -0.1008, -0.2029,  0.1698,  0.1158, -0.1945],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e83ba3e5-6548-454e-97a6-638bedc7bb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_S = 1.0\n",
    "epsilon_E = 0.1\n",
    "epsilon_decay = 100000\n",
    "\n",
    "_epsilon = lambda frame: epsilon_E + (epsilon_S - epsilon_E)*np.exp(-frame/epsilon_decay)\n",
    "plt.plot([_epsilon(frame) for frame in range(1000000)]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "648e4c1e-1e5c-405c-8e9e-0d6b47806244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce1eaf75-635a-4721-b8fb-2182fb80b18c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count():\n\u001b[0;32m---> 11\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     observation, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     13\u001b[0m     reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([reward], device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[35], line 27\u001b[0m, in \u001b[0;36mselect_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     24\u001b[0m eps_threshold \u001b[38;5;241m=\u001b[39m EPS_END \u001b[38;5;241m+\u001b[39m (EPS_START \u001b[38;5;241m-\u001b[39m EPS_END) \u001b[38;5;241m*\u001b[39m \\\n\u001b[1;32m     25\u001b[0m     math\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m*\u001b[39m steps_done \u001b[38;5;241m/\u001b[39m EPS_DECAY)\n\u001b[1;32m     26\u001b[0m steps_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample \u001b[38;5;241m>\u001b[39m \u001b[43meps\u001b[49m\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m policy_net(state)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eps' is not defined"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state = env.reset()\n",
    "    state = torch.tensor(state[0], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        #     + (1  )\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "print('Complete')\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04572da-a4c1-459b-973d-e9bc2a87210b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c6ddbd1f-3c17-47ce-a55e-691243506f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca04669c-3b75-4b07-ad85-9c0a1f0a008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078]],\n",
       "\n",
       "       [[0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078]],\n",
       "\n",
       "       [[0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078]],\n",
       "\n",
       "       [[0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078],\n",
       "        [0.70196078, 0.70196078, 0.70196078]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.2       , 0.2       , 0.2       ],\n",
       "        [0.        , 0.        , 1.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ],\n",
       "        [0.        , 1.        , 0.        ]]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nObs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d579c18-321c-4187-bc9e-2cc69415cccb",
   "metadata": {},
   "source": [
    "### Human Level Control ###\n",
    "Hier even iets anders geprobeerd maar zit nu even in between both of hier op verder of op vorige, of combinatie\n",
    "https://github.com/jihoonerd/Human-level-control-through-deep-reinforcement-learning/blob/master/dqn/agent/dqn_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4f02a-75fb-4d53-a3e5-59a324b6d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        q_value = policy_net(state).max(1)\n",
    "        return policy_net(state).max(1).indices.view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49e748-cbf8-41dd-b7f0-92c74ee8b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env=\"CommonsGame:CommonsGame-v0\"):\n",
    "        self.game_id = env\n",
    "        self.env = Environment(self.game_id, train=True)\n",
    "        self.discount_factor = 0.99\n",
    "        self.minibatch_size = 32\n",
    "        self.update_frequency = 4\n",
    "        self.target_network_update_freq = 1000\n",
    "        self.agent_history_length = 4\n",
    "        self.memory = ReplayMemory(capacity=10000, minibatch_size=self.minibatch_size)\n",
    "        self.main_network = DQN(num_actions=self.env.get_action_space_size(), hidden_size=32, output_size=8)\n",
    "        self.target_network = DQN(num_actions=self.env.get_action_space_size(), hidden_size=32, output_size=8)\n",
    "        self.optimizer = optimizer\n",
    "        self.init_epsilon = 1.0\n",
    "        self.final_epsilon = 0.1\n",
    "        self.final_explr_frame = 1000000 # nog bepalen\n",
    "        self.replay_start_size = 10000 # nog geen idee wat dit is\n",
    "        self.loss = tf.keras.losses.Huber()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.q_metric = tf.keras.metrics.Mean(name=\"Q_value\")\n",
    "        self.training_frames = int(1e7)\n",
    "        self.log_path = \"./log/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + self.game_id\n",
    "        self.summary_writer = tf.summary.create_file_writer(self.log_path + \"/summary/\")\n",
    "        self.life_game = None\n",
    "        self.print_log_interval = 10\n",
    "        self.save_weight_interval = 10\n",
    "\n",
    "        self.env.reset()\n",
    "        _, _, _, info = self.env.step(0)\n",
    "        if info[\"ale.lives\"] > 0:\n",
    "            self.life_game = True\n",
    "        else:\n",
    "            self.life_game = False   \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
